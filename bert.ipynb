{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import transformers\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 100\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-06\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./train.csv\")\n",
    "df_test = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.target\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            #pad_to_max_length=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_train[['text', 'target']].copy()\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 1)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, best_f1_score):\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float).unsqueeze(1)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%10==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if _%100==0:\n",
    "            with torch.no_grad():\n",
    "                outputs, targets = validation()\n",
    "                outputs = np.array(outputs) >= 0.5\n",
    "                accuracy = metrics.accuracy_score(targets, outputs)\n",
    "                f1_score = metrics.f1_score(targets, outputs)\n",
    "                print(f\"Accuracy Score = {accuracy}\")\n",
    "                print(f\"F1 Score = {f1_score}\")\n",
    "\n",
    "                if f1_score > best_f1_score:\n",
    "                    best_f1_score = f1_score\n",
    "                    torch.save(model.state_dict(), 'model.pt')\n",
    "                    print(\"Model saved\")\n",
    "    return best_f1_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6851012706756592\n",
      "Accuracy Score = 0.43204202232435984\n",
      "F1 Score = 0.6033929390187988\n",
      "Model saved\n",
      "Epoch: 0, Loss:  0.7208095788955688\n",
      "Epoch: 0, Loss:  0.6340247392654419\n",
      "Epoch: 0, Loss:  0.6317006349563599\n",
      "Epoch: 0, Loss:  0.7365193963050842\n",
      "Epoch: 0, Loss:  0.7259445786476135\n",
      "Epoch: 0, Loss:  0.7301448583602905\n",
      "Epoch: 0, Loss:  0.6558612585067749\n",
      "Epoch: 0, Loss:  0.7106248140335083\n",
      "Epoch: 0, Loss:  0.6737862825393677\n",
      "Epoch: 0, Loss:  0.6864073276519775\n",
      "Accuracy Score = 0.6776099803020355\n",
      "F1 Score = 0.6564030790762772\n",
      "Model saved\n",
      "Epoch: 0, Loss:  0.6575319766998291\n",
      "Epoch: 0, Loss:  0.6323495507240295\n",
      "Epoch: 0, Loss:  0.6837042570114136\n",
      "Epoch: 0, Loss:  0.5986202955245972\n",
      "Epoch: 0, Loss:  0.6646816730499268\n",
      "Epoch: 0, Loss:  0.5973669290542603\n",
      "Epoch: 0, Loss:  0.6488198041915894\n",
      "Epoch: 0, Loss:  0.6522070169448853\n",
      "Epoch: 0, Loss:  0.569665253162384\n",
      "Epoch: 0, Loss:  0.6168286800384521\n",
      "Accuracy Score = 0.7426132632961261\n",
      "F1 Score = 0.6781609195402298\n",
      "Model saved\n",
      "Epoch: 0, Loss:  0.660964846611023\n",
      "Epoch: 0, Loss:  0.7393104434013367\n",
      "Epoch: 0, Loss:  0.5252158641815186\n",
      "Epoch: 0, Loss:  0.5968858599662781\n",
      "Epoch: 0, Loss:  0.48936980962753296\n",
      "Epoch: 0, Loss:  0.5424566864967346\n",
      "Epoch: 0, Loss:  0.5251952409744263\n",
      "Epoch: 0, Loss:  0.5806559324264526\n",
      "Epoch: 0, Loss:  0.6053258180618286\n",
      "Epoch: 0, Loss:  0.47436463832855225\n",
      "Accuracy Score = 0.757715036112935\n",
      "F1 Score = 0.6714158504007124\n",
      "Epoch: 0, Loss:  0.5541001558303833\n",
      "Epoch: 0, Loss:  0.5776615738868713\n",
      "Epoch: 0, Loss:  0.5378338098526001\n",
      "Epoch: 0, Loss:  0.5299941301345825\n",
      "Epoch: 0, Loss:  0.6281968951225281\n",
      "Epoch: 0, Loss:  0.5967539548873901\n",
      "Epoch: 0, Loss:  0.6852357387542725\n",
      "Epoch: 0, Loss:  0.43416154384613037\n",
      "Epoch: 0, Loss:  0.41624853014945984\n",
      "Epoch: 0, Loss:  0.5268709659576416\n",
      "Accuracy Score = 0.7872619829284307\n",
      "F1 Score = 0.7290969899665551\n",
      "Model saved\n",
      "Epoch: 0, Loss:  0.431525319814682\n",
      "Epoch: 0, Loss:  0.35058313608169556\n",
      "Epoch: 0, Loss:  0.6699019074440002\n",
      "Epoch: 0, Loss:  0.3471164107322693\n",
      "Epoch: 0, Loss:  0.4809577763080597\n",
      "Epoch: 0, Loss:  0.46601027250289917\n",
      "Epoch: 0, Loss:  0.48159119486808777\n",
      "Epoch: 0, Loss:  0.4592651426792145\n",
      "Epoch: 0, Loss:  0.682287335395813\n",
      "Epoch: 0, Loss:  0.600238025188446\n",
      "Accuracy Score = 0.7951411687458962\n",
      "F1 Score = 0.7570093457943925\n",
      "Model saved\n",
      "Epoch: 0, Loss:  0.5334972143173218\n",
      "Epoch: 0, Loss:  0.8543920516967773\n",
      "Epoch: 0, Loss:  0.4950309991836548\n",
      "Epoch: 0, Loss:  0.7010964155197144\n",
      "Epoch: 0, Loss:  0.4714348316192627\n",
      "Epoch: 0, Loss:  0.5371772050857544\n",
      "Epoch: 0, Loss:  0.5004099011421204\n",
      "Epoch: 0, Loss:  0.45428556203842163\n",
      "Epoch: 0, Loss:  0.5607540607452393\n",
      "Epoch: 0, Loss:  0.651871919631958\n",
      "Accuracy Score = 0.8095863427445831\n",
      "F1 Score = 0.7567114093959731\n",
      "Epoch: 0, Loss:  0.4193287491798401\n",
      "Epoch: 0, Loss:  0.6767099499702454\n",
      "Epoch: 0, Loss:  0.6712453365325928\n",
      "Epoch: 0, Loss:  0.5568482279777527\n",
      "Epoch: 0, Loss:  0.4852421283721924\n",
      "Epoch: 0, Loss:  0.6111816763877869\n",
      "Epoch: 0, Loss:  0.3921170234680176\n",
      "Epoch: 0, Loss:  0.3088502883911133\n",
      "Epoch: 0, Loss:  0.5779387950897217\n",
      "Epoch: 0, Loss:  0.6074032783508301\n",
      "Accuracy Score = 0.8168089297439265\n",
      "F1 Score = 0.7751813053988719\n",
      "Model saved\n",
      "Epoch: 0, Loss:  0.2831515073776245\n",
      "Epoch: 0, Loss:  0.388967365026474\n",
      "Epoch: 0, Loss:  0.30226534605026245\n",
      "Epoch: 0, Loss:  0.6106899976730347\n",
      "Epoch: 0, Loss:  0.7826498746871948\n",
      "Epoch: 0, Loss:  0.3404689431190491\n",
      "Epoch: 1, Loss:  0.31777799129486084\n",
      "Accuracy Score = 0.8181221273801708\n",
      "F1 Score = 0.7803330689928628\n",
      "Model saved\n",
      "Epoch: 1, Loss:  0.4673875868320465\n",
      "Epoch: 1, Loss:  0.4058515727519989\n",
      "Epoch: 1, Loss:  0.278847873210907\n",
      "Epoch: 1, Loss:  0.5148581266403198\n",
      "Epoch: 1, Loss:  0.47835057973861694\n",
      "Epoch: 1, Loss:  0.37772685289382935\n",
      "Epoch: 1, Loss:  0.6230813264846802\n",
      "Epoch: 1, Loss:  0.2096201777458191\n",
      "Epoch: 1, Loss:  0.24729250371456146\n",
      "Epoch: 1, Loss:  0.6975606679916382\n",
      "Accuracy Score = 0.8253447143795141\n",
      "F1 Score = 0.7790697674418605\n",
      "Epoch: 1, Loss:  0.40874147415161133\n",
      "Epoch: 1, Loss:  0.46459388732910156\n",
      "Epoch: 1, Loss:  0.2534679174423218\n",
      "Epoch: 1, Loss:  0.5008978247642517\n",
      "Epoch: 1, Loss:  0.3582998514175415\n",
      "Epoch: 1, Loss:  0.3742310404777527\n",
      "Epoch: 1, Loss:  0.14566892385482788\n",
      "Epoch: 1, Loss:  0.825668215751648\n",
      "Epoch: 1, Loss:  0.48889946937561035\n",
      "Epoch: 1, Loss:  0.6670870780944824\n",
      "Accuracy Score = 0.824688115561392\n",
      "F1 Score = 0.7875894988066826\n",
      "Model saved\n",
      "Epoch: 1, Loss:  0.37567567825317383\n",
      "Epoch: 1, Loss:  0.293599009513855\n",
      "Epoch: 1, Loss:  0.5054145455360413\n",
      "Epoch: 1, Loss:  0.468302845954895\n",
      "Epoch: 1, Loss:  0.14452776312828064\n",
      "Epoch: 1, Loss:  0.3059200048446655\n",
      "Epoch: 1, Loss:  0.44208434224128723\n",
      "Epoch: 1, Loss:  0.31518250703811646\n",
      "Epoch: 1, Loss:  0.4000649154186249\n",
      "Epoch: 1, Loss:  0.28074392676353455\n",
      "Accuracy Score = 0.8338804990151018\n",
      "F1 Score = 0.7872161480235492\n",
      "Epoch: 1, Loss:  0.3831932246685028\n",
      "Epoch: 1, Loss:  0.39429160952568054\n",
      "Epoch: 1, Loss:  0.4918760359287262\n",
      "Epoch: 1, Loss:  0.4393165707588196\n",
      "Epoch: 1, Loss:  0.5957704782485962\n",
      "Epoch: 1, Loss:  0.3310946226119995\n",
      "Epoch: 1, Loss:  0.6489566564559937\n",
      "Epoch: 1, Loss:  0.5565740466117859\n",
      "Epoch: 1, Loss:  0.37903106212615967\n",
      "Epoch: 1, Loss:  0.20455697178840637\n",
      "Accuracy Score = 0.8338804990151018\n",
      "F1 Score = 0.8\n",
      "Model saved\n",
      "Epoch: 1, Loss:  0.5708970427513123\n",
      "Epoch: 1, Loss:  0.8545638918876648\n",
      "Epoch: 1, Loss:  0.37012824416160583\n",
      "Epoch: 1, Loss:  0.4271842837333679\n",
      "Epoch: 1, Loss:  0.35112708806991577\n",
      "Epoch: 1, Loss:  0.5451127886772156\n",
      "Epoch: 1, Loss:  0.3095948100090027\n",
      "Epoch: 1, Loss:  0.36791327595710754\n",
      "Epoch: 1, Loss:  0.3693576157093048\n",
      "Epoch: 1, Loss:  0.527281641960144\n",
      "Accuracy Score = 0.8338804990151018\n",
      "F1 Score = 0.7824591573516767\n",
      "Epoch: 1, Loss:  0.896902859210968\n",
      "Epoch: 1, Loss:  0.1361972987651825\n",
      "Epoch: 1, Loss:  0.40172016620635986\n",
      "Epoch: 1, Loss:  0.4103618264198303\n",
      "Epoch: 1, Loss:  0.22834512591362\n",
      "Epoch: 1, Loss:  0.28079932928085327\n",
      "Epoch: 1, Loss:  0.16735728085041046\n",
      "Epoch: 1, Loss:  0.3853137493133545\n",
      "Epoch: 1, Loss:  0.21867164969444275\n",
      "Epoch: 1, Loss:  0.27563565969467163\n",
      "Accuracy Score = 0.8319107025607354\n",
      "F1 Score = 0.8012422360248447\n",
      "Model saved\n",
      "Epoch: 1, Loss:  0.2915119528770447\n",
      "Epoch: 1, Loss:  0.4372338652610779\n",
      "Epoch: 1, Loss:  0.14151081442832947\n",
      "Epoch: 1, Loss:  0.34385162591934204\n",
      "Epoch: 1, Loss:  0.6465827226638794\n",
      "Epoch: 1, Loss:  0.12628909945487976\n",
      "Epoch: 1, Loss:  0.4075639843940735\n",
      "Epoch: 1, Loss:  0.38599225878715515\n",
      "Epoch: 1, Loss:  0.38208502531051636\n",
      "Epoch: 1, Loss:  0.45061683654785156\n",
      "Accuracy Score = 0.8305975049244911\n",
      "F1 Score = 0.7987519500780031\n",
      "Epoch: 1, Loss:  0.5926098823547363\n",
      "Epoch: 1, Loss:  0.4165157973766327\n",
      "Epoch: 1, Loss:  0.4941404461860657\n",
      "Epoch: 1, Loss:  0.37493669986724854\n",
      "Epoch: 1, Loss:  0.19395852088928223\n",
      "Epoch: 1, Loss:  0.18679285049438477\n",
      "Epoch: 2, Loss:  0.42495566606521606\n",
      "Accuracy Score = 0.8384766907419566\n",
      "F1 Score = 0.7996742671009772\n",
      "Epoch: 2, Loss:  0.5826966166496277\n",
      "Epoch: 2, Loss:  0.4102955460548401\n",
      "Epoch: 2, Loss:  0.21845999360084534\n",
      "Epoch: 2, Loss:  0.5890302658081055\n",
      "Epoch: 2, Loss:  0.21627216041088104\n",
      "Epoch: 2, Loss:  0.20523281395435333\n",
      "Epoch: 2, Loss:  0.21014299988746643\n",
      "Epoch: 2, Loss:  0.18654672801494598\n",
      "Epoch: 2, Loss:  0.1264149248600006\n",
      "Epoch: 2, Loss:  0.2544253170490265\n",
      "Accuracy Score = 0.8404464871963231\n",
      "F1 Score = 0.8041901692183723\n",
      "Model saved\n",
      "Epoch: 2, Loss:  0.09526969492435455\n",
      "Epoch: 2, Loss:  0.23239004611968994\n",
      "Epoch: 2, Loss:  0.6982565522193909\n",
      "Epoch: 2, Loss:  0.34830743074417114\n",
      "Epoch: 2, Loss:  0.5153509974479675\n",
      "Epoch: 2, Loss:  0.19015489518642426\n",
      "Epoch: 2, Loss:  0.3857375979423523\n",
      "Epoch: 2, Loss:  0.12365550547838211\n",
      "Epoch: 2, Loss:  0.4856679439544678\n",
      "Epoch: 2, Loss:  0.410091370344162\n",
      "Accuracy Score = 0.8437294812869337\n",
      "F1 Score = 0.8055555555555556\n",
      "Model saved\n",
      "Epoch: 2, Loss:  0.10343460738658905\n",
      "Epoch: 2, Loss:  0.5320255756378174\n",
      "Epoch: 2, Loss:  0.1549031287431717\n",
      "Epoch: 2, Loss:  0.33482062816619873\n",
      "Epoch: 2, Loss:  0.8957078456878662\n",
      "Epoch: 2, Loss:  0.12916706502437592\n",
      "Epoch: 2, Loss:  0.44853222370147705\n",
      "Epoch: 2, Loss:  0.8173143863677979\n",
      "Epoch: 2, Loss:  0.29526686668395996\n",
      "Epoch: 2, Loss:  0.18808959424495697\n",
      "Accuracy Score = 0.8424162836506894\n",
      "F1 Score = 0.8029556650246306\n",
      "Epoch: 2, Loss:  0.3719973564147949\n",
      "Epoch: 2, Loss:  0.7229747176170349\n",
      "Epoch: 2, Loss:  0.1262603998184204\n",
      "Epoch: 2, Loss:  0.2504822015762329\n",
      "Epoch: 2, Loss:  0.4392656087875366\n",
      "Epoch: 2, Loss:  0.39334380626678467\n",
      "Epoch: 2, Loss:  0.1056026741862297\n",
      "Epoch: 2, Loss:  0.3971390128135681\n",
      "Epoch: 2, Loss:  0.6230841875076294\n",
      "Epoch: 2, Loss:  0.17170219123363495\n",
      "Accuracy Score = 0.8456992777413\n",
      "F1 Score = 0.8090982940698619\n",
      "Model saved\n",
      "Epoch: 2, Loss:  0.4979003965854645\n",
      "Epoch: 2, Loss:  0.7356745004653931\n",
      "Epoch: 2, Loss:  0.25918519496917725\n",
      "Epoch: 2, Loss:  0.10970226675271988\n",
      "Epoch: 2, Loss:  0.42265430092811584\n",
      "Epoch: 2, Loss:  0.5124946236610413\n",
      "Epoch: 2, Loss:  0.43211108446121216\n",
      "Epoch: 2, Loss:  0.10422904044389725\n",
      "Epoch: 2, Loss:  0.5572575330734253\n",
      "Epoch: 2, Loss:  0.2289658784866333\n",
      "Accuracy Score = 0.8443860801050558\n",
      "F1 Score = 0.8077858880778589\n",
      "Epoch: 2, Loss:  0.3675505518913269\n",
      "Epoch: 2, Loss:  0.6643093228340149\n",
      "Epoch: 2, Loss:  0.2955954372882843\n",
      "Epoch: 2, Loss:  0.34978100657463074\n",
      "Epoch: 2, Loss:  0.8680157661437988\n",
      "Epoch: 2, Loss:  0.23721641302108765\n",
      "Epoch: 2, Loss:  0.7696951627731323\n",
      "Epoch: 2, Loss:  0.19596615433692932\n",
      "Epoch: 2, Loss:  0.128557026386261\n",
      "Epoch: 2, Loss:  0.4394757151603699\n",
      "Accuracy Score = 0.8424162836506894\n",
      "F1 Score = 0.8061389337641357\n",
      "Epoch: 2, Loss:  0.2710607051849365\n",
      "Epoch: 2, Loss:  0.11871932446956635\n",
      "Epoch: 2, Loss:  0.1867384910583496\n",
      "Epoch: 2, Loss:  0.3767411410808563\n",
      "Epoch: 2, Loss:  0.19650302827358246\n",
      "Epoch: 2, Loss:  0.9367666244506836\n",
      "Epoch: 2, Loss:  0.7183277606964111\n",
      "Epoch: 2, Loss:  0.43773114681243896\n",
      "Epoch: 2, Loss:  0.639610767364502\n",
      "Epoch: 2, Loss:  0.5759577751159668\n",
      "Accuracy Score = 0.8424162836506894\n",
      "F1 Score = 0.8045602605863192\n",
      "Epoch: 2, Loss:  0.2340770959854126\n",
      "Epoch: 2, Loss:  0.5219829082489014\n",
      "Epoch: 2, Loss:  0.3710154891014099\n",
      "Epoch: 2, Loss:  0.5922685265541077\n",
      "Epoch: 2, Loss:  0.07016672194004059\n",
      "Epoch: 2, Loss:  0.42217907309532166\n",
      "Epoch: 3, Loss:  0.1797909438610077\n",
      "Accuracy Score = 0.8411030860144452\n",
      "F1 Score = 0.8088467614533965\n",
      "Epoch: 3, Loss:  0.3691025376319885\n",
      "Epoch: 3, Loss:  0.4428253471851349\n",
      "Epoch: 3, Loss:  0.5158658623695374\n",
      "Epoch: 3, Loss:  0.48109903931617737\n",
      "Epoch: 3, Loss:  0.25254935026168823\n",
      "Epoch: 3, Loss:  0.3874116539955139\n",
      "Epoch: 3, Loss:  0.40530967712402344\n",
      "Epoch: 3, Loss:  0.3441927134990692\n",
      "Epoch: 3, Loss:  0.9237188100814819\n",
      "Epoch: 3, Loss:  0.07323914766311646\n",
      "Accuracy Score = 0.8424162836506894\n",
      "F1 Score = 0.8067632850241546\n",
      "Epoch: 3, Loss:  0.20326381921768188\n",
      "Epoch: 3, Loss:  0.2603185474872589\n",
      "Epoch: 3, Loss:  0.13739553093910217\n",
      "Epoch: 3, Loss:  0.2796701192855835\n",
      "Epoch: 3, Loss:  0.15086844563484192\n",
      "Epoch: 3, Loss:  0.11287688463926315\n",
      "Epoch: 3, Loss:  0.20381763577461243\n",
      "Epoch: 3, Loss:  0.15817660093307495\n",
      "Epoch: 3, Loss:  0.44009560346603394\n",
      "Epoch: 3, Loss:  0.3961952030658722\n",
      "Accuracy Score = 0.8430728824688115\n",
      "F1 Score = 0.8086469175340272\n",
      "Epoch: 3, Loss:  0.1264972984790802\n",
      "Epoch: 3, Loss:  0.4730185568332672\n",
      "Epoch: 3, Loss:  0.13342565298080444\n",
      "Epoch: 3, Loss:  0.24818193912506104\n",
      "Epoch: 3, Loss:  0.25192660093307495\n",
      "Epoch: 3, Loss:  0.22949573397636414\n",
      "Epoch: 3, Loss:  0.18508866429328918\n",
      "Epoch: 3, Loss:  0.4619573950767517\n",
      "Epoch: 3, Loss:  0.09071464836597443\n",
      "Epoch: 3, Loss:  0.24677112698554993\n",
      "Accuracy Score = 0.8463558765594222\n",
      "F1 Score = 0.8151658767772512\n",
      "Model saved\n",
      "Epoch: 3, Loss:  0.42844635248184204\n",
      "Epoch: 3, Loss:  0.11306639015674591\n",
      "Epoch: 3, Loss:  0.06786738336086273\n",
      "Epoch: 3, Loss:  0.09387114644050598\n",
      "Epoch: 3, Loss:  0.4044318199157715\n",
      "Epoch: 3, Loss:  0.15934261679649353\n",
      "Epoch: 3, Loss:  0.18689779937267303\n",
      "Epoch: 3, Loss:  0.46326977014541626\n",
      "Epoch: 3, Loss:  0.49034664034843445\n",
      "Epoch: 3, Loss:  0.22017326951026917\n",
      "Accuracy Score = 0.8443860801050558\n",
      "F1 Score = 0.810853950518755\n",
      "Epoch: 3, Loss:  0.5095618963241577\n",
      "Epoch: 3, Loss:  0.7068537473678589\n",
      "Epoch: 3, Loss:  0.5109592080116272\n",
      "Epoch: 3, Loss:  0.8967005014419556\n",
      "Epoch: 3, Loss:  0.28173547983169556\n",
      "Epoch: 3, Loss:  0.22174867987632751\n",
      "Epoch: 3, Loss:  0.15632233023643494\n",
      "Epoch: 3, Loss:  0.12708142399787903\n",
      "Epoch: 3, Loss:  0.13147282600402832\n",
      "Epoch: 3, Loss:  0.162014901638031\n",
      "Accuracy Score = 0.8411030860144452\n",
      "F1 Score = 0.8029315960912052\n",
      "Epoch: 3, Loss:  0.2465927004814148\n",
      "Epoch: 3, Loss:  0.1561616063117981\n",
      "Epoch: 3, Loss:  0.8387472629547119\n",
      "Epoch: 3, Loss:  0.2739642262458801\n",
      "Epoch: 3, Loss:  0.29557308554649353\n",
      "Epoch: 3, Loss:  0.41700005531311035\n",
      "Epoch: 3, Loss:  0.356902539730072\n",
      "Epoch: 3, Loss:  0.3102429509162903\n",
      "Epoch: 3, Loss:  0.14728301763534546\n",
      "Epoch: 3, Loss:  0.3309245705604553\n",
      "Accuracy Score = 0.8463558765594222\n",
      "F1 Score = 0.8094462540716613\n",
      "Epoch: 3, Loss:  0.0778256207704544\n",
      "Epoch: 3, Loss:  0.07665900141000748\n",
      "Epoch: 3, Loss:  0.12037427723407745\n",
      "Epoch: 3, Loss:  0.5936833620071411\n",
      "Epoch: 3, Loss:  0.11947892606258392\n",
      "Epoch: 3, Loss:  1.003960132598877\n",
      "Epoch: 3, Loss:  0.2205950915813446\n",
      "Epoch: 3, Loss:  0.216488778591156\n",
      "Epoch: 3, Loss:  0.5467373728752136\n",
      "Epoch: 3, Loss:  0.18142063915729523\n",
      "Accuracy Score = 0.8437294812869337\n",
      "F1 Score = 0.8140625\n",
      "Epoch: 3, Loss:  0.11292150616645813\n",
      "Epoch: 3, Loss:  0.19918231666088104\n",
      "Epoch: 3, Loss:  0.4760591387748718\n",
      "Epoch: 3, Loss:  0.21196883916854858\n",
      "Epoch: 3, Loss:  0.11641547828912735\n",
      "Epoch: 3, Loss:  0.24942556023597717\n",
      "Epoch: 4, Loss:  0.23666267096996307\n",
      "Accuracy Score = 0.845042678923178\n",
      "F1 Score = 0.8084415584415584\n",
      "Epoch: 4, Loss:  0.32675668597221375\n",
      "Epoch: 4, Loss:  0.11645583808422089\n",
      "Epoch: 4, Loss:  0.32621967792510986\n",
      "Epoch: 4, Loss:  0.3101484179496765\n",
      "Epoch: 4, Loss:  0.24568811058998108\n",
      "Epoch: 4, Loss:  0.08670689165592194\n",
      "Epoch: 4, Loss:  0.5086201429367065\n",
      "Epoch: 4, Loss:  0.7867528200149536\n",
      "Epoch: 4, Loss:  0.1417582631111145\n",
      "Epoch: 4, Loss:  0.4268573820590973\n",
      "Accuracy Score = 0.8470124753775443\n",
      "F1 Score = 0.8122481869460113\n",
      "Epoch: 4, Loss:  0.42608729004859924\n",
      "Epoch: 4, Loss:  0.392009437084198\n",
      "Epoch: 4, Loss:  0.0875161737203598\n",
      "Epoch: 4, Loss:  0.42609643936157227\n",
      "Epoch: 4, Loss:  0.2109459936618805\n",
      "Epoch: 4, Loss:  0.07522332668304443\n",
      "Epoch: 4, Loss:  0.39452463388442993\n",
      "Epoch: 4, Loss:  0.07752163708209991\n",
      "Epoch: 4, Loss:  0.13579177856445312\n",
      "Epoch: 4, Loss:  0.09321314096450806\n",
      "Accuracy Score = 0.8437294812869337\n",
      "F1 Score = 0.810207336523126\n",
      "Epoch: 4, Loss:  0.5124495029449463\n",
      "Epoch: 4, Loss:  0.13596931099891663\n",
      "Epoch: 4, Loss:  0.0899248942732811\n",
      "Epoch: 4, Loss:  0.7073350548744202\n",
      "Epoch: 4, Loss:  0.20909635722637177\n",
      "Epoch: 4, Loss:  0.17289340496063232\n",
      "Epoch: 4, Loss:  0.15822702646255493\n",
      "Epoch: 4, Loss:  0.42038875818252563\n",
      "Epoch: 4, Loss:  0.4137685298919678\n",
      "Epoch: 4, Loss:  0.15045441687107086\n",
      "Accuracy Score = 0.8463558765594222\n",
      "F1 Score = 0.8160377358490566\n",
      "Model saved\n",
      "Epoch: 4, Loss:  0.47023534774780273\n",
      "Epoch: 4, Loss:  0.11912253499031067\n",
      "Epoch: 4, Loss:  0.207785502076149\n",
      "Epoch: 4, Loss:  0.37326309084892273\n",
      "Epoch: 4, Loss:  0.3538203537464142\n",
      "Epoch: 4, Loss:  0.13128337264060974\n",
      "Epoch: 4, Loss:  0.577851414680481\n",
      "Epoch: 4, Loss:  0.052268676459789276\n",
      "Epoch: 4, Loss:  0.3918835520744324\n",
      "Epoch: 4, Loss:  0.2570514380931854\n",
      "Accuracy Score = 0.845042678923178\n",
      "F1 Score = 0.8105939004815409\n",
      "Epoch: 4, Loss:  0.5234732627868652\n",
      "Epoch: 4, Loss:  0.1097404956817627\n",
      "Epoch: 4, Loss:  0.10061226785182953\n",
      "Epoch: 4, Loss:  0.23554369807243347\n",
      "Epoch: 4, Loss:  0.15680637955665588\n",
      "Epoch: 4, Loss:  0.11198630183935165\n",
      "Epoch: 4, Loss:  0.5965597629547119\n",
      "Epoch: 4, Loss:  0.30044594407081604\n",
      "Epoch: 4, Loss:  0.40115389227867126\n",
      "Epoch: 4, Loss:  0.06874474883079529\n",
      "Accuracy Score = 0.8437294812869337\n",
      "F1 Score = 0.8089887640449438\n",
      "Epoch: 4, Loss:  0.03766964003443718\n",
      "Epoch: 4, Loss:  0.45798566937446594\n",
      "Epoch: 4, Loss:  0.11950342357158661\n",
      "Epoch: 4, Loss:  0.29240888357162476\n",
      "Epoch: 4, Loss:  0.3505490720272064\n",
      "Epoch: 4, Loss:  0.10728871077299118\n",
      "Epoch: 4, Loss:  0.056349046528339386\n",
      "Epoch: 4, Loss:  0.3857702910900116\n",
      "Epoch: 4, Loss:  0.8936104774475098\n",
      "Epoch: 4, Loss:  0.3516128659248352\n",
      "Accuracy Score = 0.8463558765594222\n",
      "F1 Score = 0.8136942675159236\n",
      "Epoch: 4, Loss:  0.4734034836292267\n",
      "Epoch: 4, Loss:  0.2734442949295044\n",
      "Epoch: 4, Loss:  0.36568135023117065\n",
      "Epoch: 4, Loss:  0.35143789649009705\n",
      "Epoch: 4, Loss:  0.1811608076095581\n",
      "Epoch: 4, Loss:  0.27983343601226807\n",
      "Epoch: 4, Loss:  0.1477670967578888\n",
      "Epoch: 4, Loss:  0.5144838094711304\n",
      "Epoch: 4, Loss:  0.17564578354358673\n",
      "Epoch: 4, Loss:  0.39981135725975037\n",
      "Accuracy Score = 0.8509520682862771\n",
      "F1 Score = 0.8155970755483347\n",
      "Epoch: 4, Loss:  0.3590242266654968\n",
      "Epoch: 4, Loss:  0.6244459748268127\n",
      "Epoch: 4, Loss:  0.4581716060638428\n",
      "Epoch: 4, Loss:  0.06830308586359024\n",
      "Epoch: 4, Loss:  0.08012133836746216\n",
      "Epoch: 4, Loss:  0.3254550099372864\n",
      "Epoch: 5, Loss:  0.6000040769577026\n",
      "Accuracy Score = 0.8463558765594222\n",
      "F1 Score = 0.8078817733990148\n",
      "Epoch: 5, Loss:  0.4242122173309326\n",
      "Epoch: 5, Loss:  0.3795783817768097\n",
      "Epoch: 5, Loss:  0.034969378262758255\n",
      "Epoch: 5, Loss:  0.18880794942378998\n",
      "Epoch: 5, Loss:  0.433350145816803\n",
      "Epoch: 5, Loss:  0.06448020040988922\n",
      "Epoch: 5, Loss:  0.15185759961605072\n",
      "Epoch: 5, Loss:  0.17440667748451233\n",
      "Epoch: 5, Loss:  0.09427423030138016\n",
      "Epoch: 5, Loss:  0.6976430416107178\n",
      "Accuracy Score = 0.8476690741956664\n",
      "F1 Score = 0.8132045088566827\n",
      "Epoch: 5, Loss:  0.15611344575881958\n",
      "Epoch: 5, Loss:  0.09640834480524063\n",
      "Epoch: 5, Loss:  0.34211888909339905\n",
      "Epoch: 5, Loss:  0.1092449501156807\n",
      "Epoch: 5, Loss:  0.07922594249248505\n",
      "Epoch: 5, Loss:  0.4949371814727783\n",
      "Epoch: 5, Loss:  0.17546316981315613\n",
      "Epoch: 5, Loss:  0.28157925605773926\n",
      "Epoch: 5, Loss:  0.13995103538036346\n",
      "Epoch: 5, Loss:  0.04806708171963692\n",
      "Accuracy Score = 0.8443860801050558\n",
      "F1 Score = 0.8114558472553699\n",
      "Epoch: 5, Loss:  0.12288469821214676\n",
      "Epoch: 5, Loss:  0.22322767972946167\n",
      "Epoch: 5, Loss:  0.24969826638698578\n",
      "Epoch: 5, Loss:  0.3096889853477478\n",
      "Epoch: 5, Loss:  0.08893204480409622\n",
      "Epoch: 5, Loss:  0.3285672068595886\n",
      "Epoch: 5, Loss:  0.09247121214866638\n",
      "Epoch: 5, Loss:  0.6184875965118408\n",
      "Epoch: 5, Loss:  1.3411895036697388\n",
      "Epoch: 5, Loss:  0.06794954836368561\n",
      "Accuracy Score = 0.8463558765594222\n",
      "F1 Score = 0.8174726989079563\n",
      "Model saved\n",
      "Epoch: 5, Loss:  0.23373883962631226\n",
      "Epoch: 5, Loss:  0.04768954589962959\n",
      "Epoch: 5, Loss:  0.2461422234773636\n",
      "Epoch: 5, Loss:  0.06704393029212952\n",
      "Epoch: 5, Loss:  0.27023911476135254\n",
      "Epoch: 5, Loss:  0.3809523582458496\n",
      "Epoch: 5, Loss:  0.17573754489421844\n",
      "Epoch: 5, Loss:  0.3886358439922333\n",
      "Epoch: 5, Loss:  0.07043572515249252\n",
      "Epoch: 5, Loss:  0.6060178279876709\n",
      "Accuracy Score = 0.8437294812869337\n",
      "F1 Score = 0.8123028391167192\n",
      "Epoch: 5, Loss:  0.04451228678226471\n",
      "Epoch: 5, Loss:  0.3313591480255127\n",
      "Epoch: 5, Loss:  0.12177975475788116\n",
      "Epoch: 5, Loss:  0.07885324954986572\n",
      "Epoch: 5, Loss:  0.18115147948265076\n",
      "Epoch: 5, Loss:  0.07177864760160446\n",
      "Epoch: 5, Loss:  0.12468573451042175\n",
      "Epoch: 5, Loss:  0.6095104813575745\n",
      "Epoch: 5, Loss:  0.39621827006340027\n",
      "Epoch: 5, Loss:  0.37992918491363525\n",
      "Accuracy Score = 0.845042678923178\n",
      "F1 Score = 0.8121019108280255\n",
      "Epoch: 5, Loss:  0.08825093507766724\n",
      "Epoch: 5, Loss:  0.07303688675165176\n",
      "Epoch: 5, Loss:  0.5111657381057739\n",
      "Epoch: 5, Loss:  0.05954650789499283\n",
      "Epoch: 5, Loss:  0.18082879483699799\n",
      "Epoch: 5, Loss:  0.3244868516921997\n",
      "Epoch: 5, Loss:  0.6592089533805847\n",
      "Epoch: 5, Loss:  0.34999555349349976\n",
      "Epoch: 5, Loss:  0.5640920400619507\n",
      "Epoch: 5, Loss:  0.3753831088542938\n",
      "Accuracy Score = 0.8437294812869337\n",
      "F1 Score = 0.8125984251968504\n",
      "Epoch: 5, Loss:  0.5320707559585571\n",
      "Epoch: 5, Loss:  0.3250173032283783\n",
      "Epoch: 5, Loss:  0.1882772445678711\n",
      "Epoch: 5, Loss:  0.05747289955615997\n",
      "Epoch: 5, Loss:  0.08253198862075806\n",
      "Epoch: 5, Loss:  0.1930118203163147\n",
      "Epoch: 5, Loss:  0.47981974482536316\n",
      "Epoch: 5, Loss:  0.3933548629283905\n",
      "Epoch: 5, Loss:  0.43791142106056213\n",
      "Epoch: 5, Loss:  0.10255225747823715\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m best_f1_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 4\u001b[0m     best_f1_score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_f1_score\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, best_f1_score)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 22\u001b[0m         outputs, targets \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(outputs) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     24\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39maccuracy_score(targets, outputs)\n",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     10\u001b[0m targets \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m fin_targets\u001b[38;5;241m.\u001b[39mextend(targets\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     13\u001b[0m fin_outputs\u001b[38;5;241m.\u001b[39mextend(torch\u001b[38;5;241m.\u001b[39msigmoid(outputs)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mBERTClass.forward\u001b[0;34m(self, ids, mask, token_type_ids)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids, mask, token_type_ids):\n\u001b[0;32m----> 9\u001b[0m     _, output_1\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     output_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2(output_1)\n\u001b[1;32m     11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml3(output_2)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:309\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[0;32m--> 309\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    311\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    313\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_f1_score = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    best_f1_score = train(epoch, best_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
